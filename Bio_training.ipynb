{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bio_training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marel88/Bioinfo_7_1/blob/master/Bio_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "G0EP9-6VtU73",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**DATASET PRE PROCESSING**"
      ]
    },
    {
      "metadata": {
        "id": "H4oUSSOuy7VP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install py_wsi --no-index --find-links file:///Users/Mac/Desktop/py-wsi-1.1.zip\n",
        "!apt install openslide-tool\n",
        "!pip install openslide-python\n",
        "\n",
        "#SCRIPT FOR IMAGE CROPPING\n",
        "\n",
        "\n",
        "import py_wsi\n",
        "import py_wsi.imagepy_toolkit as tk\n",
        "from py_wsi import turtle\n",
        "\n",
        "file_dir = \"/Users/Mac/Desktop/ROI-dataset-bioinf/Training/Healthy/\"\n",
        "db_location = \"/Users/Mac/Desktop/ROI-dataset-bioinf/Training/Healthy_patches/\"\n",
        "xml_dir = file_dir\n",
        "patch_size = 64\n",
        "level = 10\n",
        "db_name = \"\"\n",
        "overlap = 0\n",
        "\n",
        "# All possible labels mapped to integer ids in order of increasing severity.\n",
        "label_map = {}\n",
        "\n",
        "turtle = turtle.Turtle(file_dir, db_location, db_name, xml_dir=xml_dir, label_map=label_map, storage_type='disk')\n",
        "turtle.sample_and_store_patches(patch_size, level, overlap, load_xml=False, limit_bounds=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eiShUK7CtamG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**CNN BASE**"
      ]
    },
    {
      "metadata": {
        "id": "g3BQ4dD9unk5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#CNN BASE MODEL - DATA LOADING\n",
        "\n",
        "import numpy as np\n",
        "!pip3 install keras\n",
        "!pip3 install tensorflow==1.5.0\n",
        "!pip install mxnet-mkl\n",
        "!pip3 install sklearn\n",
        "!pip3 install keras_tqdm\n",
        "!pip install scipy\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation\n",
        "from keras.layers.core import Dense, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import *\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import *\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NaQkjeNR7MnV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_path=\"/Users/Mac/Desktop/ROI-dataset-bioinf/Training/Patches/\"\n",
        "valid_path=\"/Users/Mac/Desktop/ROI-dataset-bioinf/Validation/Patches\"\n",
        "test_path =\"/Users/Mac/Desktop/ROI-dataset-bioinf/Test/Patches\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UzyWGHvlC_Fb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(224,224), classes=['Benign', 'Healthy', 'Cancer'], batch_size=100)\n",
        "valid_batches = ImageDataGenerator().flow_from_directory(valid_path, target_size=(224,224), classes=['Benign', 'Healthy', 'Cancer'], batch_size=100)\n",
        "test_batches = ImageDataGenerator().flow_from_directory(test_path, target_size=(224,224), classes=['Benign', 'Healthy', 'Cancer'], batch_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5RLPLu5mmhGj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#For plotting images with labels\n",
        "def plots(ims, figsize=(12,6), rows=1, interp=False, titles=None):\n",
        "    if type(ims[0]) is np.ndarray:\n",
        "        ims = np.array(ims).astype(np.uint8)\n",
        "        if (ims.shape[-1] != 3):\n",
        "            ims = ims.transpose((0,2,3,1))\n",
        "    f = plt.figure(figsize=figsize)\n",
        "    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n",
        "    for i in range(len(ims)):\n",
        "        sp = f.add_subplot(rows, cols, i+1)\n",
        "        sp.axis('Off')\n",
        "        if titles is not None:\n",
        "            sp.set_title(titles[i], fontsize=16)\n",
        "        plt.imshow(ims[i], interpolation=None if interp else 'none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NyJOfCvIVQWS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "imgs, labels = next(train_batches)\n",
        "\n",
        "plots(imgs,titles=labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QBTKs_NaVR86",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#CNN MODEL BUILDING\n",
        "model = Sequential([Conv2D(32,(3,3),activation='relu',input_shape=(64,64,3)), Flatten(), Dense(3, activation='softmax'),])\n",
        "model.compile(Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit_generator(train_batches, steps_per_epoch=22, validation_data=valid_batches, validation_steps=22, epochs=5, verbose=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "97pjhfx3WmVq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_imgs, test_labels = next(test_batches)\n",
        "plots(test_imgs, titles = test_labels)\n",
        "#test_labels = test_labels[:,0]\n",
        "test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DMrMcizcaTrF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict_generator(test_batches, steps=1, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ze9ZQv93dGX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sMFD6PUqdIHM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install -U scikit-learn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(test_labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yB1x83jJb8N8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#MULTI-LABEL CONFUSION MATRIX\n",
        "\n",
        "y_test_non_category = [ np.argmax(t) for t in test_labels ]\n",
        "y_predict_non_category = [ np.argmax(t) for t in predictions ]\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f-e8J4TceQl-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm,classes,normalize=False,title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "\n",
        "#This function prints and plots the confusion matrix.\n",
        "#Normalization can ben applied by setting 'normalize-True'\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks=np.arange(len(classes))\n",
        "    plt.xticks(tick_marks,classes,rotation=45)\n",
        "    plt.yticks(tick_marks,classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2\n",
        "    for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i,j], horizontalalignment=\"center\", color=\"white\" if cm[i,j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RzLMUdm-keV_",
        "colab_type": "code",
        "outputId": "881cf8f9-953f-4102-bdbf-22ecf589f77a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "cell_type": "code",
      "source": [
        "cm_plot_labels = ['Benign','Healthy','Cancer']\n",
        "plot_confusion_matrix(conf_mat, cm_plot_labels,title ='Confusion Matrix')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[20  0  0]\n",
            " [63  0  0]\n",
            " [17  0  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "kPrSiv3lmHC3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FIN QUI TUTTO OK, IL MODELLO FA SCHIFO MA TECNICAMENTE FA QUELLO CHE DEVE FARE, SE LA CONFUSION MATRIX È GIUSTA, HO TROVATO COME PLOTTARLA CON UN MODELLO A MULTI LABEL**"
      ]
    },
    {
      "metadata": {
        "id": "dH8ZmCrhtf1j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FINE TUNING OF VGG16** "
      ]
    },
    {
      "metadata": {
        "id": "024xq-WbmSqU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#FINE TUNING OF A PRE-TRAINED MODEL (VGG16)\n",
        "\n",
        "vgg16_model = keras.applications.vgg16.VGG16()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LKxIhkFQpEoZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vgg16_model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0rV5hG5K1ftf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "type(vgg16_model)\n",
        "#create a Sequential model in which we add all the layers of the VGG16 model\n",
        "model = Sequential()\n",
        "for layer in vgg16_model.layers[:-1]:\n",
        "  #all layers except for the last one which is the predictions layers we don't want\n",
        "    model.add(layer)\n",
        "    \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e_uGdPHX2kX9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for layer in model.layers:\n",
        "    layer.trainable = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JI2uQhG_Cfiv",
        "colab_type": "code",
        "outputId": "47b5b2b9-fb27-4ad0-afb3-4d27fbf9f6da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "cell_type": "code",
      "source": [
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 12291     \n",
            "=================================================================\n",
            "Total params: 134,272,835\n",
            "Trainable params: 12,291\n",
            "Non-trainable params: 134,260,544\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "95lCz-aEDjZc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TRAIN THE VGG16 MODEL\n",
        "\n",
        "model.compile(Adam(lr=.0001), loss ='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit_generator(train_batches, steps_per_epoch=100, validation_data=valid_batches, validation_steps=100, epochs=5, verbose=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SsJ3c_txJc7S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**DA QUI IN POI NON HO PIU' PROVATO PERCHÈ IL MIO PC NON REGGE IL TRAINING E QUINDI BISOGNA RICORRERE AL CLUSTER**"
      ]
    },
    {
      "metadata": {
        "id": "1iEgYFqaJory",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_imgs, test_labels = next(test_batches)\n",
        "plots(test_imgs, titles = test_labels)\n",
        "test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16yYjozTaS3Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict_generator(test_batches, steps=1, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7iGp5W2baaEN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install -U scikit-learn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(test_labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lXWPqShJahcX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#MULTI-LABEL CONFUSION MATRIX\n",
        "\n",
        "y_test_non_category = [ np.argmax(t) for t in test_labels ]\n",
        "y_predict_non_category = [ np.argmax(t) for t in predictions ]\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OV8hA-HYan44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cm_plot_labels = ['Benign','Healthy','Cancer']\n",
        "plot_confusion_matrix(conf_mat, cm_plot_labels,title ='Confusion Matrix')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ebt5IeUAbJGI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**QUESTA DOVREBBE ESSERE LA FINE DELLA PRIMA PARTE DI TEST, SE FIN QUI I RISULTATI SONO BUONI, POSSIAMO FERMARCI SE NO BISOGNA FARE DATA AUGMENTATION E AUMENTARE LE EPOCHS**"
      ]
    },
    {
      "metadata": {
        "id": "K8dBilwSbHUR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#DATA AUGMENATION\n",
        "\n",
        "gen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.15, zoom_range=0.1, channel_shift_range=10., horizontal_flip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ddwqK8VM3M8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_path = \"INSERT PATH TO IMAGE\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7s1uj2ZKRIRi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = np.expand_dims(ndimage.imread(image_path),0)\n",
        "plt.imshow(image[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mIb3_hoZRZd3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Generates batches of augmented images from on image\n",
        "\n",
        "aug_iter = gen.flow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-HvKA8JYRhCf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get 10 samples of augmented images\n",
        "aug_image = [next(aug_iter)[0].astype(np.uint8) for i in range(10)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UDHtej_pRtX3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plots(aug_images, figsize=(20,7), rows=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gMCxAa6Pky9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**SE I RISULTATI OTTENUTI CON UN DETERMINATO SET DI IMPOSTAZIONI SONO BUONI E VOGLIAMO FREEZZARLI E RIPRODURLI ...**"
      ]
    },
    {
      "metadata": {
        "id": "Wf1Il5dSPwvq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "#Setting the seed for numpy-generated random numbers\n",
        "np.random.seed(37)\n",
        "\n",
        "#Setting the seed for Python random numbers\n",
        "rn.seed(1254)\n",
        "\n",
        "#Setting the seed for Tensorflow random numbers\n",
        "tf.set_random_seed(89)\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "#Force TensorFlow to use a single thread\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "#Paste training Keras code here after setting the random seeds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hzocYylhSBvu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " **CLASS ACTIVION MAP**"
      ]
    },
    {
      "metadata": {
        "id": "_u4vOjUh_zfO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# --- CLASS ACTIVATION MAP --- #\n",
        "from keras.models import *\n",
        "from keras.callbacks import *\n",
        "import keras.backend as K\n",
        "from model import *\n",
        "from data import *\n",
        "import cv2\n",
        "\n",
        "#After the last convolutional layer in a typical network like VGG16, we have an N-dimensional image, where N is the number of filters in this layer. \n",
        "#For example in VGG16, the last convolutional layer has 512 filters. For example, for an 1024x1024 input image (lets discard the fully connected layers, \n",
        "#so we can use any input image size we want), the output shape of the last convolutional layer will be 512x64x64. \n",
        "#Since 1024/64 = 16, we have a 16x16 spatial mapping resolution. \n",
        "#A global average pooling (GAP) layer just takes each of these 512 channels, and returns their spatial average. \n",
        "#Channels with high activations, will have high signals.\n",
        "\n",
        "def global_average_pooling(x):\n",
        "        return K.mean(x, axis = (2, 3))\n",
        "  \n",
        "\n",
        "def global_average_pooling_shape(input_shape):\n",
        "        return input_shape[0:2]\n",
        "  \n",
        "#The second step is to assign a weight to each output from the global average pooling layer, for each of the categories. \n",
        "#This can be done by adding a dense linear layer + softmax, training an SVM on the GAP output, or applying any other linear classifier on top of the GAP. \n",
        "#These weights set the importance of each of the convolutional layer outputs.\n",
        "\n",
        "\n",
        "#TO DO: \n",
        "#    --- definire una funzione che crei il modello VGG16 (ho visto che Elena ha creato il modello ma non dentro una funzione)\n",
        "def get_model():\n",
        "\t    model = VGG16_convolutions()\n",
        "\t    model = load_model_weights(model, \"vgg16_weights.h5\")\n",
        "\t    \n",
        "\t    model.add(Lambda(global_average_pooling, \n",
        "\t              output_shape=global_average_pooling_shape))\n",
        "\t    model.add(Dense(2, activation = 'softmax', init='uniform'))\n",
        "\t    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.5, nesterov=True)\n",
        "\t    model.compile(loss = 'categorical_crossentropy', \\\n",
        "            optimizer = sgd, metrics=['accuracy'])\n",
        "\t    return model\n",
        "\n",
        "def load_model_weights(model, weights_path):\n",
        "    print 'Loading model.'\n",
        "    f = h5py.File(weights_path)\n",
        "    for k in range(f.attrs['nb_layers']):\n",
        "        if k >= len(model.layers):\n",
        "            # we don't look at the last (fully-connected) layers in the savefile\n",
        "            break\n",
        "        g = f['layer_{}'.format(k)]\n",
        "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
        "        model.layers[k].set_weights(weights)\n",
        "        model.layers[k].trainable = False\n",
        "    f.close()\n",
        "    print 'Model loaded.'\n",
        "    return model\n",
        "\n",
        "def get_output_layer(model, layer_name):\n",
        "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "    layer = layer_dict[layer_name]\n",
        "    return layer    \n",
        "       \n",
        "#TO DO: \n",
        "# --- definire il \"dataset_path\"\n",
        "# --- definire la funzione \"load_images\": è necessario creare due path diversi, uno per immagini positive e l'altro per quelle negative\n",
        "#     (poi la faccio io appena riusciamo a fare i test)\n",
        "\n",
        "def train(dataset_path):\n",
        "        model = get_model()\n",
        "        X, y = load_images(dataset_path)\n",
        "\t      print \"Training..\"\n",
        "        checkpoint_path=\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "        checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto')\n",
        "        model.fit(X, y, nb_epoch=40, batch_size=32, validation_split=0.2, verbose=1, callbacks=[checkpoint])\n",
        "\n",
        "#Now to create a heatmap for a class we can just take output images from the last convolutional layer, multiply them by their assigned weights \n",
        "#(different weights for each class), and sum.\n",
        "\n",
        "def visualize_class_activation_map(model_path, img_path, output_path):\n",
        "        model = load_model(model_path)\n",
        "        original_img = cv2.imread(img_path, 1)\n",
        "        width, height, _ = original_img.shape\n",
        "\n",
        "        #Reshape to the network input shape (3, w, h).\n",
        "        img = np.array([np.transpose(np.float32(original_img), (2, 0, 1))])\n",
        "        \n",
        "        #Get the 512 input weights to the softmax.\n",
        "        class_weights = model.layers[-1].get_weights()[0]\n",
        "        final_conv_layer = get_output_layer(model, \"conv5_3\")\n",
        "        get_output = K.function([model.layers[0].input], \\\n",
        "                    [final_conv_layer.output, \n",
        "        model.layers[-1].output])\n",
        "        [conv_outputs, predictions] = get_output([img])\n",
        "        conv_outputs = conv_outputs[0, :, :, :]\n",
        "\n",
        "        #Create the class activation map.\n",
        "        cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[1:3])\n",
        "        target_class = 1\n",
        "        for i, w in enumerate(class_weights[:, target_class]):\n",
        "                cam += w * conv_outputs[i, :, :]\n",
        "        print \"predictions\", predictions\n",
        "        cam /= np.max(cam)\n",
        "        cam = cv2.resize(cam, (height, width))\n",
        "        heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
        "        heatmap[np.where(cam < 0.2)] = 0\n",
        "        img = heatmap*0.5 + original_img\n",
        "        cv2.imwrite(output_path, img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dHRqp23fGCzq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}