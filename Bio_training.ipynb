{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bio_training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marel88/Bioinfo_7_1/blob/master/Bio_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LzELL8aM7GgM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FOR GOOGLE DRIVE IMPORT**"
      ]
    },
    {
      "metadata": {
        "id": "u3ONVKn-7eZI",
        "colab_type": "code",
        "outputId": "4ca47d50-c839-4b05-8c5c-59f640806f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G0EP9-6VtU73",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**DATASET PRE PROCESSING**"
      ]
    },
    {
      "metadata": {
        "id": "H4oUSSOuy7VP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install py_wsi --no-index --find-links file:///Users/Mac/Desktop/py-wsi-1.1.zip\n",
        "!apt install openslide-tool\n",
        "!pip install openslide-python\n",
        "\n",
        "#SCRIPT FOR IMAGE CROPPING\n",
        "\n",
        "\n",
        "import py_wsi\n",
        "import py_wsi.imagepy_toolkit as tk\n",
        "from py_wsi import turtle\n",
        "\n",
        "file_dir = \"/Users/Mac/Desktop/ROI-dataset-bioinf/Training/Healthy/\"\n",
        "db_location = \"/Users/Mac/Desktop/ROI-dataset-bioinf/Training/Healthy_patches/\"\n",
        "xml_dir = file_dir\n",
        "patch_size = 64\n",
        "level = 10\n",
        "db_name = \"\"\n",
        "overlap = 0\n",
        "\n",
        "# All possible labels mapped to integer ids in order of increasing severity.\n",
        "label_map = {}\n",
        "\n",
        "turtle = turtle.Turtle(file_dir, db_location, db_name, xml_dir=xml_dir, label_map=label_map, storage_type='disk')\n",
        "turtle.sample_and_store_patches(patch_size, level, overlap, load_xml=False, limit_bounds=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XaISfS_fvzVz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**SCRIPT FOR ERASING ONLY WHITE IMAGES**"
      ]
    },
    {
      "metadata": {
        "id": "fuEFrnvMv5bi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#the function for deleting white patches\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def deleteWhite(path):                                              #path is the path of the Patches folder\n",
        "    \"\"\"Function that deletes all patches that are completely (or mostly) white.\"\"\"\n",
        "\n",
        "    folders = [path + \"/Healthy\", path + \"/Benign\", path + \"/Cancer\"]\n",
        "    for folder in folders:                                          #loops through the three folders of patches\n",
        "        count = 0\n",
        "        for filename in glob.glob(os.path.join(folder, '*.png')):   #sequentially selects each .png file in the current folder\n",
        "            img = cv2.imread(filename)\n",
        "            (x,y,z) = img.shape                                     #img.shap is 64 64 3\n",
        "            white = 0\n",
        "            for i in range(0,x):                                    #analyzes the image pixel by pixel\n",
        "                for j in range(0,y):\n",
        "                    pixel = img[i,j]\n",
        "                    b = pixel[0]\n",
        "                    g = pixel[1]\n",
        "                    r = pixel[2]\n",
        "                    if b == 255:                                    #if b,g,r are all 255 it means the pixel is white\n",
        "                        if g == 255:\n",
        "                            if r == 255:\n",
        "                                white = white + 1                   #I use white as a counter to keep track of the number of white pixels\n",
        "                    if white > 50:                                  #in the dataset that we have, an image having a few white pixels means it will be almost all white\n",
        "                        break                                       #thus to make the process a bit shorter I only count to 50 white pixels and then break the loop, since it is unneded to count further\n",
        "                if white > 50:\n",
        "                    break\n",
        "            if white > 50:                                          #if a patch is found to have many white pixels (which means it will be mostly white) I delete it from its folder\n",
        "                os.remove(filename)\n",
        "                count = count + 1\n",
        "        print(\"Deleted\",count,\"patches in\",folder)\n",
        "\n",
        "    return;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b44RtPXH6tkR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#LOOP TO CLEAN UP FOLDERS\n",
        "\n",
        "main_path=\"/content/gdrive/My Drive/Bioinformatica/Organized dataset\"\n",
        "main_folders = [main_path + \"/Test/Patches\", main_path + \"/Training/Patches\", main_path + \"/Validation/Patches\"]\n",
        "for main_folder in main_folders:\n",
        "    new_path = main_folder\n",
        "    deleteWhite(new_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eiShUK7CtamG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**CNN BASE**"
      ]
    },
    {
      "metadata": {
        "id": "g3BQ4dD9unk5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#CNN BASE MODEL - DATA LOADING\n",
        "\n",
        "import numpy as np\n",
        "!pip3 install keras\n",
        "#!pip3 install tensorflow==1.5.0\n",
        "!pip3 install tensorflow\n",
        "!pip install mxnet-mkl\n",
        "!pip3 install sklearn\n",
        "!pip3 install keras_tqdm\n",
        "!pip install scipy\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation\n",
        "from keras.layers.core import Dense, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import *\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import *\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NaQkjeNR7MnV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_path=\"/content/gdrive/My Drive/Bioinformatica/Organized dataset/Training/Patches\"\n",
        "#train_path=\"/Users/Mac/Desktop/ROI-dataset-bioinf/Training/Patches/\"\n",
        "valid_path=\"/content/gdrive/My Drive/Bioinformatica/Organized dataset/Validation/Patches\"\n",
        "#valid_path=\"/Users/Mac/Desktop/ROI-dataset-bioinf/Validation/Patches\"\n",
        "#test_path =\"/Users/Mac/Desktop/ROI-dataset-bioinf/Test/Patches\"\n",
        "test_path=\"/content/gdrive/My Drive/Bioinformatica/Organized dataset/Test/Patches\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UzyWGHvlC_Fb",
        "colab_type": "code",
        "outputId": "2eb1e3c9-aa69-4985-a142-04b9f2f6a3c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "train_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(224,224), classes=['Benign', 'Healthy', 'Cancer'], batch_size=100)\n",
        "valid_batches = ImageDataGenerator().flow_from_directory(valid_path, target_size=(224,224), classes=['Benign', 'Healthy', 'Cancer'], batch_size=100)\n",
        "test_batches = ImageDataGenerator().flow_from_directory(test_path, target_size=(224,224), classes=['Benign', 'Healthy', 'Cancer'], batch_size=100)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3306 images belonging to 3 classes.\n",
            "Found 1589 images belonging to 3 classes.\n",
            "Found 765 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5RLPLu5mmhGj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#For plotting images with labels\n",
        "def plots(ims, figsize=(12,6), rows=1, interp=False, titles=None):\n",
        "    if type(ims[0]) is np.ndarray:\n",
        "        ims = np.array(ims).astype(np.uint8)\n",
        "        if (ims.shape[-1] != 3):\n",
        "            ims = ims.transpose((0,2,3,1))\n",
        "    f = plt.figure(figsize=figsize)\n",
        "    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n",
        "    for i in range(len(ims)):\n",
        "        sp = f.add_subplot(rows, cols, i+1)\n",
        "        sp.axis('Off')\n",
        "        if titles is not None:\n",
        "            sp.set_title(titles[i], fontsize=16)\n",
        "        plt.imshow(ims[i], interpolation=None if interp else 'none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NyJOfCvIVQWS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "imgs, labels = next(train_batches)\n",
        "\n",
        "#plots(imgs,titles=labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QBTKs_NaVR86",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#CNN MODEL BUILDING\n",
        "model = Sequential([Conv2D(32,(3,3),activation='relu',input_shape=(64,64,3)), Flatten(), Dense(3, activation='softmax'),])\n",
        "model.compile(Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit_generator(train_batches, steps_per_epoch=22, validation_data=valid_batches, validation_steps=22, epochs=5, verbose=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "97pjhfx3WmVq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_imgs, test_labels = next(test_batches)\n",
        "plots(test_imgs, titles = test_labels)\n",
        "#test_labels = test_labels[:,0]\n",
        "test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DMrMcizcaTrF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict_generator(test_batches, steps=1, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ze9ZQv93dGX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sMFD6PUqdIHM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install -U scikit-learn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(test_labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yB1x83jJb8N8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#MULTI-LABEL CONFUSION MATRIX\n",
        "\n",
        "y_test_non_category = [ np.argmax(t) for t in test_labels ]\n",
        "y_predict_non_category = [ np.argmax(t) for t in predictions ]\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f-e8J4TceQl-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm,classes,normalize=True,title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "\n",
        "#This function prints and plots the confusion matrix.\n",
        "#Normalization can ben applied by setting 'normalize-True'\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks=np.arange(len(classes))\n",
        "    plt.xticks(tick_marks,classes,rotation=45)\n",
        "    plt.yticks(tick_marks,classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2\n",
        "    for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, \"{:0.2f}\".format(cm[i, j]), horizontalalignment=\"center\", color=\"white\" if cm[i,j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RzLMUdm-keV_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cm_plot_labels = ['Benign','Healthy','Cancer']\n",
        "plot_confusion_matrix(conf_mat, cm_plot_labels,title ='Confusion Matrix')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPrSiv3lmHC3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FIN QUI TUTTO OK, IL MODELLO FA SCHIFO MA TECNICAMENTE FA QUELLO CHE DEVE FARE, SE LA CONFUSION MATRIX È GIUSTA, HO TROVATO COME PLOTTARLA CON UN MODELLO A MULTI LABEL**"
      ]
    },
    {
      "metadata": {
        "id": "dH8ZmCrhtf1j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FINE TUNING OF VGG16** "
      ]
    },
    {
      "metadata": {
        "id": "024xq-WbmSqU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#FINE TUNING OF A PRE-TRAINED MODEL (VGG16)\n",
        "\n",
        "vgg16_model = keras.applications.vgg16.VGG16()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LKxIhkFQpEoZ",
        "colab_type": "code",
        "outputId": "7a7d51bd-32bd-4e4f-bd5c-3169422008a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "cell_type": "code",
      "source": [
        "vgg16_model.summary()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0rV5hG5K1ftf",
        "colab_type": "code",
        "outputId": "6775719a-4690-4602-c47e-9fdb0209e614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "cell_type": "code",
      "source": [
        "type(vgg16_model)\n",
        "#create a Sequential model in which we add all the layers of the VGG16 model\n",
        "model = Sequential()\n",
        "for layer in vgg16_model.layers[:-1]:\n",
        "  #all layers except for the last one which is the predictions layers we don't want\n",
        "    model.add(layer)\n",
        "    \n",
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "=================================================================\n",
            "Total params: 134,260,544\n",
            "Trainable params: 134,260,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e_uGdPHX2kX9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for layer in model.layers:\n",
        "    layer.trainable = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JI2uQhG_Cfiv",
        "colab_type": "code",
        "outputId": "8537fd94-d363-4f47-d4df-afb4bf51d611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "cell_type": "code",
      "source": [
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12291     \n",
            "=================================================================\n",
            "Total params: 134,272,835\n",
            "Trainable params: 12,291\n",
            "Non-trainable params: 134,260,544\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "95lCz-aEDjZc",
        "colab_type": "code",
        "outputId": "593d3f62-7a38-42a9-dbb3-26f8f933e020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "#TRAIN THE VGG16 MODEL\n",
        "es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                              min_delta=0,\n",
        "                              patience=2,\n",
        "                              verbose=0, mode='auto')\n",
        "model.compile(Adam(lr=.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0), loss ='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit_generator(train_batches, steps_per_epoch=100, validation_data=valid_batches, validation_steps=100, epochs=15, verbose=2, callbacks=[es])\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            " - 105s - loss: 0.8570 - acc: 0.7048 - val_loss: 0.5479 - val_acc: 0.7932\n",
            "Epoch 2/15\n",
            " - 105s - loss: 0.3131 - acc: 0.8853 - val_loss: 0.4706 - val_acc: 0.8347\n",
            "Epoch 3/15\n",
            " - 105s - loss: 0.2430 - acc: 0.9122 - val_loss: 0.4187 - val_acc: 0.8597\n",
            "Epoch 4/15\n",
            " - 105s - loss: 0.2039 - acc: 0.9258 - val_loss: 0.3925 - val_acc: 0.8607\n",
            "Epoch 5/15\n",
            " - 105s - loss: 0.1814 - acc: 0.9346 - val_loss: 0.3741 - val_acc: 0.8662\n",
            "Epoch 6/15\n",
            " - 105s - loss: 0.1636 - acc: 0.9421 - val_loss: 0.3584 - val_acc: 0.8753\n",
            "Epoch 7/15\n",
            " - 105s - loss: 0.1548 - acc: 0.9410 - val_loss: 0.3510 - val_acc: 0.8813\n",
            "Epoch 8/15\n",
            " - 105s - loss: 0.1374 - acc: 0.9511 - val_loss: 0.3347 - val_acc: 0.8883\n",
            "Epoch 9/15\n",
            " - 105s - loss: 0.1280 - acc: 0.9541 - val_loss: 0.3408 - val_acc: 0.8804\n",
            "Epoch 10/15\n",
            " - 105s - loss: 0.1180 - acc: 0.9563 - val_loss: 0.3365 - val_acc: 0.8860\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fab3259e978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "1iEgYFqaJory",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_imgs, test_labels = next(test_batches)\n",
        "#plots(test_imgs, titles = test_labels)\n",
        "#test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16yYjozTaS3Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict_generator(test_batches, steps=1, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7iGp5W2baaEN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install -U scikit-learn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#cm = confusion_matrix(test_labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lXWPqShJahcX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#MULTI-LABEL CONFUSION MATRIX\n",
        "\n",
        "y_test_non_category = [ np.argmax(t) for t in test_labels ]\n",
        "y_predict_non_category = [ np.argmax(t) for t in predictions ]\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OV8hA-HYan44",
        "colab_type": "code",
        "outputId": "8bd28d72-9c0e-4eee-9ecd-90023abe126b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "cell_type": "code",
      "source": [
        "cm_plot_labels = ['Benign','Healthy','Cancer']\n",
        "plot_confusion_matrix(conf_mat, cm_plot_labels,title ='Confusion Matrix')\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalized confusion matrix\n",
            "[[0.22222222 0.66666667 0.11111111]\n",
            " [0.3030303  0.60606061 0.09090909]\n",
            " [0.         0.75       0.25      ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAEmCAYAAADmw8JdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ebt5IeUAbJGI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**QUESTA DOVREBBE ESSERE LA FINE DELLA PRIMA PARTE DI TEST, SE FIN QUI I RISULTATI SONO BUONI, POSSIAMO FERMARCI SE NO BISOGNA FARE DATA AUGMENTATION E AUMENTARE LE EPOCHS**"
      ]
    },
    {
      "metadata": {
        "id": "K8dBilwSbHUR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#DATA AUGMENATION\n",
        "\n",
        "gen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.15, zoom_range=0.1, channel_shift_range=10., horizontal_flip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ddwqK8VM3M8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_path = \"INSERT PATH TO IMAGE\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7s1uj2ZKRIRi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = np.expand_dims(ndimage.imread(image_path),0)\n",
        "plt.imshow(image[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mIb3_hoZRZd3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Generates batches of augmented images from an image\n",
        "\n",
        "aug_iter = gen.flow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-HvKA8JYRhCf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get 10 samples of augmented images\n",
        "aug_image = [next(aug_iter)[0].astype(np.uint8) for i in range(10)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UDHtej_pRtX3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plots(aug_images, figsize=(20,7), rows=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gMCxAa6Pky9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**SE I RISULTATI OTTENUTI CON UN DETERMINATO SET DI IMPOSTAZIONI SONO BUONI E VOGLIAMO FREEZZARLI E RIPRODURLI ...**"
      ]
    },
    {
      "metadata": {
        "id": "Wf1Il5dSPwvq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "#Setting the seed for numpy-generated random numbers\n",
        "np.random.seed(37)\n",
        "\n",
        "#Setting the seed for Python random numbers\n",
        "rn.seed(1254)\n",
        "\n",
        "#Setting the seed for Tensorflow random numbers\n",
        "tf.set_random_seed(89)\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "#Force TensorFlow to use a single thread\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "#Paste training Keras code here after setting the random seeds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hzocYylhSBvu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " **CLASS ACTIVION MAP**"
      ]
    },
    {
      "metadata": {
        "id": "_u4vOjUh_zfO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# --- CLASS ACTIVATION MAP --- #\n",
        "from keras.models import *\n",
        "from keras.callbacks import *\n",
        "import keras.backend as K\n",
        "from model import *\n",
        "from data import *\n",
        "import cv2\n",
        "\n",
        "#After the last convolutional layer in a typical network like VGG16, we have an N-dimensional image, where N is the number of filters in this layer. \n",
        "#For example in VGG16, the last convolutional layer has 512 filters. For example, for an 1024x1024 input image (lets discard the fully connected layers, \n",
        "#so we can use any input image size we want), the output shape of the last convolutional layer will be 512x64x64. \n",
        "#Since 1024/64 = 16, we have a 16x16 spatial mapping resolution. \n",
        "#A global average pooling (GAP) layer just takes each of these 512 channels, and returns their spatial average. \n",
        "#Channels with high activations, will have high signals.\n",
        "\n",
        "def global_average_pooling(x):\n",
        "        return K.mean(x, axis = (2, 3))\n",
        "  \n",
        "\n",
        "def global_average_pooling_shape(input_shape):\n",
        "        return input_shape[0:2]\n",
        "  \n",
        "#The second step is to assign a weight to each output from the global average pooling layer, for each of the categories. \n",
        "#This can be done by adding a dense linear layer + softmax, training an SVM on the GAP output, or applying any other linear classifier on top of the GAP. \n",
        "#These weights set the importance of each of the convolutional layer outputs.\n",
        "\n",
        "\n",
        "#TO DO: \n",
        "#    --- definire una funzione che crei il modello VGG16 (ho visto che Elena ha creato il modello ma non dentro una funzione)\n",
        "def get_model():\n",
        "\t    model = VGG16_convolutions()\n",
        "\t    model = load_model_weights(model, \"vgg16_weights.h5\")\n",
        "\t    \n",
        "\t    model.add(Lambda(global_average_pooling, \n",
        "\t              output_shape=global_average_pooling_shape))\n",
        "\t    model.add(Dense(2, activation = 'softmax', init='uniform'))\n",
        "\t    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.5, nesterov=True)\n",
        "\t    model.compile(loss = 'categorical_crossentropy', \\\n",
        "            optimizer = sgd, metrics=['accuracy'])\n",
        "\t    return model\n",
        "\n",
        "def load_model_weights(model, weights_path):\n",
        "    print 'Loading model.'\n",
        "    f = h5py.File(weights_path)\n",
        "    for k in range(f.attrs['nb_layers']):\n",
        "        if k >= len(model.layers):\n",
        "            # we don't look at the last (fully-connected) layers in the savefile\n",
        "            break\n",
        "        g = f['layer_{}'.format(k)]\n",
        "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
        "        model.layers[k].set_weights(weights)\n",
        "        model.layers[k].trainable = False\n",
        "    f.close()\n",
        "    print 'Model loaded.'\n",
        "    return model\n",
        "\n",
        "def get_output_layer(model, layer_name):\n",
        "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "    layer = layer_dict[layer_name]\n",
        "    return layer    \n",
        "       \n",
        "#TO DO: \n",
        "# --- definire il \"dataset_path\"\n",
        "# --- definire la funzione \"load_images\": è necessario creare due path diversi, uno per immagini positive e l'altro per quelle negative\n",
        "#     (poi la faccio io appena riusciamo a fare i test)\n",
        "\n",
        "def train(dataset_path):\n",
        "        model = get_model()\n",
        "        X, y = load_images(dataset_path)\n",
        "\t      print \"Training..\"\n",
        "        checkpoint_path=\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "        checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto')\n",
        "        model.fit(X, y, nb_epoch=40, batch_size=32, validation_split=0.2, verbose=1, callbacks=[checkpoint])\n",
        "\n",
        "#Now to create a heatmap for a class we can just take output images from the last convolutional layer, multiply them by their assigned weights \n",
        "#(different weights for each class), and sum.\n",
        "\n",
        "def visualize_class_activation_map(model_path, img_path, output_path):\n",
        "        model = load_model(model_path)\n",
        "        original_img = cv2.imread(img_path, 1)\n",
        "        width, height, _ = original_img.shape\n",
        "\n",
        "        #Reshape to the network input shape (3, w, h).\n",
        "        img = np.array([np.transpose(np.float32(original_img), (2, 0, 1))])\n",
        "        \n",
        "        #Get the 512 input weights to the softmax.\n",
        "        class_weights = model.layers[-1].get_weights()[0]\n",
        "        final_conv_layer = get_output_layer(model, \"conv5_3\")\n",
        "        get_output = K.function([model.layers[0].input], \\\n",
        "                    [final_conv_layer.output, \n",
        "        model.layers[-1].output])\n",
        "        [conv_outputs, predictions] = get_output([img])\n",
        "        conv_outputs = conv_outputs[0, :, :, :]\n",
        "\n",
        "        #Create the class activation map.\n",
        "        cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[1:3])\n",
        "        target_class = 1\n",
        "        for i, w in enumerate(class_weights[:, target_class]):\n",
        "                cam += w * conv_outputs[i, :, :]\n",
        "        print \"predictions\", predictions\n",
        "        cam /= np.max(cam)\n",
        "        cam = cv2.resize(cam, (height, width))\n",
        "        heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
        "        heatmap[np.where(cam < 0.2)] = 0\n",
        "        img = heatmap*0.5 + original_img\n",
        "        cv2.imwrite(output_path, img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dHRqp23fGCzq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}