{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bio_training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marel88/Bioinfo_7_1/blob/master/Bio_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LzELL8aM7GgM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FOR GOOGLE DRIVE IMPORT**"
      ]
    },
    {
      "metadata": {
        "id": "u3ONVKn-7eZI",
        "colab_type": "code",
        "outputId": "4ca47d50-c839-4b05-8c5c-59f640806f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G0EP9-6VtU73",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**DATASET PRE PROCESSING**"
      ]
    },
    {
      "metadata": {
        "id": "H4oUSSOuy7VP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install py_wsi --no-index --find-links file:///Users/Mac/Desktop/py-wsi-1.1.zip\n",
        "!apt install openslide-tool\n",
        "!pip install openslide-python\n",
        "\n",
        "#SCRIPT FOR IMAGE CROPPING\n",
        "\n",
        "\n",
        "import py_wsi\n",
        "import py_wsi.imagepy_toolkit as tk\n",
        "from py_wsi import turtle\n",
        "\n",
        "file_dir = \"/Users/Mac/Desktop/ROI-dataset-bioinf/Training/Healthy/\"\n",
        "db_location = \"/Users/Mac/Desktop/ROI-dataset-bioinf/Training/Healthy_patches/\"\n",
        "xml_dir = file_dir\n",
        "patch_size = 64\n",
        "level = 10\n",
        "db_name = \"\"\n",
        "overlap = 0\n",
        "\n",
        "# All possible labels mapped to integer ids in order of increasing severity.\n",
        "label_map = {}\n",
        "\n",
        "turtle = turtle.Turtle(file_dir, db_location, db_name, xml_dir=xml_dir, label_map=label_map, storage_type='disk')\n",
        "turtle.sample_and_store_patches(patch_size, level, overlap, load_xml=False, limit_bounds=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XaISfS_fvzVz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**SCRIPT FOR ERASING ONLY WHITE IMAGES**"
      ]
    },
    {
      "metadata": {
        "id": "fuEFrnvMv5bi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#the function for deleting white patches\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def deleteWhite(path):                                              #path is the path of the Patches folder\n",
        "    \"\"\"Function that deletes all patches that are completely (or mostly) white.\"\"\"\n",
        "\n",
        "    folders = [path + \"/Healthy\", path + \"/Benign\", path + \"/Cancer\"]\n",
        "    for folder in folders:                                          #loops through the three folders of patches\n",
        "        count = 0\n",
        "        for filename in glob.glob(os.path.join(folder, '*.png')):   #sequentially selects each .png file in the current folder\n",
        "            img = cv2.imread(filename)\n",
        "            (x,y,z) = img.shape                                     #img.shap is 64 64 3\n",
        "            white = 0\n",
        "            for i in range(0,x):                                    #analyzes the image pixel by pixel\n",
        "                for j in range(0,y):\n",
        "                    pixel = img[i,j]\n",
        "                    b = pixel[0]\n",
        "                    g = pixel[1]\n",
        "                    r = pixel[2]\n",
        "                    if b == 255:                                    #if b,g,r are all 255 it means the pixel is white\n",
        "                        if g == 255:\n",
        "                            if r == 255:\n",
        "                                white = white + 1                   #I use white as a counter to keep track of the number of white pixels\n",
        "                    if white > 50:                                  #in the dataset that we have, an image having a few white pixels means it will be almost all white\n",
        "                        break                                       #thus to make the process a bit shorter I only count to 50 white pixels and then break the loop, since it is unneded to count further\n",
        "                if white > 50:\n",
        "                    break\n",
        "            if white > 50:                                          #if a patch is found to have many white pixels (which means it will be mostly white) I delete it from its folder\n",
        "                os.remove(filename)\n",
        "                count = count + 1\n",
        "        print(\"Deleted\",count,\"patches in\",folder)\n",
        "\n",
        "    return;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b44RtPXH6tkR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#LOOP TO CLEAN UP FOLDERS\n",
        "\n",
        "main_path=\"/content/gdrive/My Drive/Bioinformatica/Organized dataset\"\n",
        "main_folders = [main_path + \"/Test/Patches\", main_path + \"/Training/Patches\", main_path + \"/Validation/Patches\"]\n",
        "for main_folder in main_folders:\n",
        "    new_path = main_folder\n",
        "    deleteWhite(new_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eiShUK7CtamG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**CNN BASE**"
      ]
    },
    {
      "metadata": {
        "id": "g3BQ4dD9unk5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#CNN BASE MODEL - DATA LOADING\n",
        "\n",
        "import numpy as np\n",
        "!pip3 install keras\n",
        "#!pip3 install tensorflow==1.5.0\n",
        "!pip3 install tensorflow\n",
        "!pip install mxnet-mkl\n",
        "!pip3 install sklearn\n",
        "!pip3 install keras_tqdm\n",
        "!pip install scipy\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation\n",
        "from keras.layers.core import Dense, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import *\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import *\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NaQkjeNR7MnV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_path=\"/content/gdrive/My Drive/Bioinformatica/Organized dataset/Training/Patches\"\n",
        "#train_path=\"/Users/Mac/Desktop/ROI-dataset-bioinf/Training/Patches/\"\n",
        "valid_path=\"/content/gdrive/My Drive/Bioinformatica/Organized dataset/Validation/Patches\"\n",
        "#valid_path=\"/Users/Mac/Desktop/ROI-dataset-bioinf/Validation/Patches\"\n",
        "#test_path =\"/Users/Mac/Desktop/ROI-dataset-bioinf/Test/Patches\"\n",
        "test_path=\"/content/gdrive/My Drive/Bioinformatica/Organized dataset/Test/Patches\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UzyWGHvlC_Fb",
        "colab_type": "code",
        "outputId": "2eb1e3c9-aa69-4985-a142-04b9f2f6a3c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "train_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(224,224), classes=['Benign', 'Healthy', 'Cancer'], batch_size=100)\n",
        "valid_batches = ImageDataGenerator().flow_from_directory(valid_path, target_size=(224,224), classes=['Benign', 'Healthy', 'Cancer'], batch_size=100)\n",
        "test_batches = ImageDataGenerator().flow_from_directory(test_path, target_size=(224,224), classes=['Benign', 'Healthy', 'Cancer'], batch_size=100)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3306 images belonging to 3 classes.\n",
            "Found 1589 images belonging to 3 classes.\n",
            "Found 765 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5RLPLu5mmhGj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#For plotting images with labels\n",
        "def plots(ims, figsize=(12,6), rows=1, interp=False, titles=None):\n",
        "    if type(ims[0]) is np.ndarray:\n",
        "        ims = np.array(ims).astype(np.uint8)\n",
        "        if (ims.shape[-1] != 3):\n",
        "            ims = ims.transpose((0,2,3,1))\n",
        "    f = plt.figure(figsize=figsize)\n",
        "    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n",
        "    for i in range(len(ims)):\n",
        "        sp = f.add_subplot(rows, cols, i+1)\n",
        "        sp.axis('Off')\n",
        "        if titles is not None:\n",
        "            sp.set_title(titles[i], fontsize=16)\n",
        "        plt.imshow(ims[i], interpolation=None if interp else 'none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NyJOfCvIVQWS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "imgs, labels = next(train_batches)\n",
        "\n",
        "#plots(imgs,titles=labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QBTKs_NaVR86",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#CNN MODEL BUILDING\n",
        "model = Sequential([Conv2D(32,(3,3),activation='relu',input_shape=(64,64,3)), Flatten(), Dense(3, activation='softmax'),])\n",
        "model.compile(Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit_generator(train_batches, steps_per_epoch=22, validation_data=valid_batches, validation_steps=22, epochs=5, verbose=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "97pjhfx3WmVq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_imgs, test_labels = next(test_batches)\n",
        "plots(test_imgs, titles = test_labels)\n",
        "#test_labels = test_labels[:,0]\n",
        "test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DMrMcizcaTrF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict_generator(test_batches, steps=1, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ze9ZQv93dGX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sMFD6PUqdIHM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install -U scikit-learn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(test_labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yB1x83jJb8N8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#MULTI-LABEL CONFUSION MATRIX\n",
        "\n",
        "y_test_non_category = [ np.argmax(t) for t in test_labels ]\n",
        "y_predict_non_category = [ np.argmax(t) for t in predictions ]\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f-e8J4TceQl-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm,classes,normalize=False,title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "\n",
        "#This function prints and plots the confusion matrix.\n",
        "#Normalization can ben applied by setting 'normalize-True'\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks=np.arange(len(classes))\n",
        "    plt.xticks(tick_marks,classes,rotation=45)\n",
        "    plt.yticks(tick_marks,classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2\n",
        "    for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i,j], horizontalalignment=\"center\", color=\"white\" if cm[i,j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RzLMUdm-keV_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cm_plot_labels = ['Benign','Healthy','Cancer']\n",
        "plot_confusion_matrix(conf_mat, cm_plot_labels,title ='Confusion Matrix')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPrSiv3lmHC3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FIN QUI TUTTO OK, IL MODELLO FA SCHIFO MA TECNICAMENTE FA QUELLO CHE DEVE FARE, SE LA CONFUSION MATRIX È GIUSTA, HO TROVATO COME PLOTTARLA CON UN MODELLO A MULTI LABEL**"
      ]
    },
    {
      "metadata": {
        "id": "dH8ZmCrhtf1j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FINE TUNING OF VGG16** "
      ]
    },
    {
      "metadata": {
        "id": "024xq-WbmSqU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#FINE TUNING OF A PRE-TRAINED MODEL (VGG16)\n",
        "\n",
        "vgg16_model = keras.applications.vgg16.VGG16()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LKxIhkFQpEoZ",
        "colab_type": "code",
        "outputId": "7a7d51bd-32bd-4e4f-bd5c-3169422008a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "cell_type": "code",
      "source": [
        "vgg16_model.summary()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0rV5hG5K1ftf",
        "colab_type": "code",
        "outputId": "6775719a-4690-4602-c47e-9fdb0209e614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "cell_type": "code",
      "source": [
        "type(vgg16_model)\n",
        "#create a Sequential model in which we add all the layers of the VGG16 model\n",
        "model = Sequential()\n",
        "for layer in vgg16_model.layers[:-1]:\n",
        "  #all layers except for the last one which is the predictions layers we don't want\n",
        "    model.add(layer)\n",
        "    \n",
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "=================================================================\n",
            "Total params: 134,260,544\n",
            "Trainable params: 134,260,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e_uGdPHX2kX9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for layer in model.layers:\n",
        "    layer.trainable = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JI2uQhG_Cfiv",
        "colab_type": "code",
        "outputId": "8537fd94-d363-4f47-d4df-afb4bf51d611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "cell_type": "code",
      "source": [
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12291     \n",
            "=================================================================\n",
            "Total params: 134,272,835\n",
            "Trainable params: 12,291\n",
            "Non-trainable params: 134,260,544\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "95lCz-aEDjZc",
        "colab_type": "code",
        "outputId": "593d3f62-7a38-42a9-dbb3-26f8f933e020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "#TRAIN THE VGG16 MODEL\n",
        "es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                              min_delta=0,\n",
        "                              patience=2,\n",
        "                              verbose=0, mode='auto')\n",
        "model.compile(Adam(lr=.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0), loss ='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit_generator(train_batches, steps_per_epoch=100, validation_data=valid_batches, validation_steps=100, epochs=15, verbose=2, callbacks=[es])\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            " - 105s - loss: 0.8570 - acc: 0.7048 - val_loss: 0.5479 - val_acc: 0.7932\n",
            "Epoch 2/15\n",
            " - 105s - loss: 0.3131 - acc: 0.8853 - val_loss: 0.4706 - val_acc: 0.8347\n",
            "Epoch 3/15\n",
            " - 105s - loss: 0.2430 - acc: 0.9122 - val_loss: 0.4187 - val_acc: 0.8597\n",
            "Epoch 4/15\n",
            " - 105s - loss: 0.2039 - acc: 0.9258 - val_loss: 0.3925 - val_acc: 0.8607\n",
            "Epoch 5/15\n",
            " - 105s - loss: 0.1814 - acc: 0.9346 - val_loss: 0.3741 - val_acc: 0.8662\n",
            "Epoch 6/15\n",
            " - 105s - loss: 0.1636 - acc: 0.9421 - val_loss: 0.3584 - val_acc: 0.8753\n",
            "Epoch 7/15\n",
            " - 105s - loss: 0.1548 - acc: 0.9410 - val_loss: 0.3510 - val_acc: 0.8813\n",
            "Epoch 8/15\n",
            " - 105s - loss: 0.1374 - acc: 0.9511 - val_loss: 0.3347 - val_acc: 0.8883\n",
            "Epoch 9/15\n",
            " - 105s - loss: 0.1280 - acc: 0.9541 - val_loss: 0.3408 - val_acc: 0.8804\n",
            "Epoch 10/15\n",
            " - 105s - loss: 0.1180 - acc: 0.9563 - val_loss: 0.3365 - val_acc: 0.8860\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fab3259e978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "1iEgYFqaJory",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_imgs, test_labels = next(test_batches)\n",
        "#plots(test_imgs, titles = test_labels)\n",
        "#test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16yYjozTaS3Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict_generator(test_batches, steps=1, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7iGp5W2baaEN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install -U scikit-learn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#cm = confusion_matrix(test_labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lXWPqShJahcX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#MULTI-LABEL CONFUSION MATRIX\n",
        "\n",
        "y_test_non_category = [ np.argmax(t) for t in test_labels ]\n",
        "y_predict_non_category = [ np.argmax(t) for t in predictions ]\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "conf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OV8hA-HYan44",
        "colab_type": "code",
        "outputId": "e3099ffd-f086-40a5-f919-9de96dde6036",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "cell_type": "code",
      "source": [
        "cm_plot_labels = ['Benign','Healthy','Cancer']\n",
        "plot_confusion_matrix(conf_mat, cm_plot_labels,title ='Confusion Matrix')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[ 4 12  2]\n",
            " [20 40  6]\n",
            " [ 0 12  4]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAEmCAYAAADmw8JdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/mgs9N5e2VEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ebt5IeUAbJGI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**QUESTA DOVREBBE ESSERE LA FINE DELLA PRIMA PARTE DI TEST, SE FIN QUI I RISULTATI SONO BUONI, POSSIAMO FERMARCI SE NO BISOGNA FARE DATA AUGMENTATION E AUMENTARE LE EPOCHS**"
      ]
    },
    {
      "metadata": {
        "id": "K8dBilwSbHUR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#DATA AUGMENATION\n",
        "\n",
        "gen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.15, zoom_range=0.1, channel_shift_range=10., horizontal_flip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ddwqK8VM3M8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_path = \"INSERT PATH TO IMAGE\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7s1uj2ZKRIRi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = np.expand_dims(ndimage.imread(image_path),0)\n",
        "plt.imshow(image[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mIb3_hoZRZd3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Generates batches of augmented images from an image\n",
        "\n",
        "aug_iter = gen.flow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-HvKA8JYRhCf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get 10 samples of augmented images\n",
        "aug_image = [next(aug_iter)[0].astype(np.uint8) for i in range(10)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UDHtej_pRtX3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plots(aug_images, figsize=(20,7), rows=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gMCxAa6Pky9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**SE I RISULTATI OTTENUTI CON UN DETERMINATO SET DI IMPOSTAZIONI SONO BUONI E VOGLIAMO FREEZZARLI E RIPRODURLI ...**"
      ]
    },
    {
      "metadata": {
        "id": "Wf1Il5dSPwvq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "#Setting the seed for numpy-generated random numbers\n",
        "np.random.seed(37)\n",
        "\n",
        "#Setting the seed for Python random numbers\n",
        "rn.seed(1254)\n",
        "\n",
        "#Setting the seed for Tensorflow random numbers\n",
        "tf.set_random_seed(89)\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "#Force TensorFlow to use a single thread\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "#Paste training Keras code here after setting the random seeds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hzocYylhSBvu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " **CLASS ACTIVION MAP**"
      ]
    },
    {
      "metadata": {
        "id": "_u4vOjUh_zfO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# --- CLASS ACTIVATION MAP --- #\n",
        "from keras.models import *\n",
        "from keras.callbacks import *\n",
        "import keras.backend as K\n",
        "from model import *\n",
        "from data import *\n",
        "import cv2\n",
        "\n",
        "#After the last convolutional layer in a typical network like VGG16, we have an N-dimensional image, where N is the number of filters in this layer. \n",
        "#For example in VGG16, the last convolutional layer has 512 filters. For example, for an 1024x1024 input image (lets discard the fully connected layers, \n",
        "#so we can use any input image size we want), the output shape of the last convolutional layer will be 512x64x64. \n",
        "#Since 1024/64 = 16, we have a 16x16 spatial mapping resolution. \n",
        "#A global average pooling (GAP) layer just takes each of these 512 channels, and returns their spatial average. \n",
        "#Channels with high activations, will have high signals.\n",
        "\n",
        "def global_average_pooling(x):\n",
        "        return K.mean(x, axis = (2, 3))\n",
        "  \n",
        "\n",
        "def global_average_pooling_shape(input_shape):\n",
        "        return input_shape[0:2]\n",
        "  \n",
        "#The second step is to assign a weight to each output from the global average pooling layer, for each of the categories. \n",
        "#This can be done by adding a dense linear layer + softmax, training an SVM on the GAP output, or applying any other linear classifier on top of the GAP. \n",
        "#These weights set the importance of each of the convolutional layer outputs.\n",
        "\n",
        "\n",
        "#TO DO: \n",
        "#    --- definire una funzione che crei il modello VGG16 (ho visto che Elena ha creato il modello ma non dentro una funzione)\n",
        "def get_model():\n",
        "\t    model = VGG16_convolutions()\n",
        "\t    model = load_model_weights(model, \"vgg16_weights.h5\")\n",
        "\t    \n",
        "\t    model.add(Lambda(global_average_pooling, \n",
        "\t              output_shape=global_average_pooling_shape))\n",
        "\t    model.add(Dense(2, activation = 'softmax', init='uniform'))\n",
        "\t    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.5, nesterov=True)\n",
        "\t    model.compile(loss = 'categorical_crossentropy', \\\n",
        "            optimizer = sgd, metrics=['accuracy'])\n",
        "\t    return model\n",
        "\n",
        "def load_model_weights(model, weights_path):\n",
        "    print 'Loading model.'\n",
        "    f = h5py.File(weights_path)\n",
        "    for k in range(f.attrs['nb_layers']):\n",
        "        if k >= len(model.layers):\n",
        "            # we don't look at the last (fully-connected) layers in the savefile\n",
        "            break\n",
        "        g = f['layer_{}'.format(k)]\n",
        "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
        "        model.layers[k].set_weights(weights)\n",
        "        model.layers[k].trainable = False\n",
        "    f.close()\n",
        "    print 'Model loaded.'\n",
        "    return model\n",
        "\n",
        "def get_output_layer(model, layer_name):\n",
        "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "    layer = layer_dict[layer_name]\n",
        "    return layer    \n",
        "       \n",
        "#TO DO: \n",
        "# --- definire il \"dataset_path\"\n",
        "# --- definire la funzione \"load_images\": è necessario creare due path diversi, uno per immagini positive e l'altro per quelle negative\n",
        "#     (poi la faccio io appena riusciamo a fare i test)\n",
        "\n",
        "def train(dataset_path):\n",
        "        model = get_model()\n",
        "        X, y = load_images(dataset_path)\n",
        "\t      print \"Training..\"\n",
        "        checkpoint_path=\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "        checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto')\n",
        "        model.fit(X, y, nb_epoch=40, batch_size=32, validation_split=0.2, verbose=1, callbacks=[checkpoint])\n",
        "\n",
        "#Now to create a heatmap for a class we can just take output images from the last convolutional layer, multiply them by their assigned weights \n",
        "#(different weights for each class), and sum.\n",
        "\n",
        "def visualize_class_activation_map(model_path, img_path, output_path):\n",
        "        model = load_model(model_path)\n",
        "        original_img = cv2.imread(img_path, 1)\n",
        "        width, height, _ = original_img.shape\n",
        "\n",
        "        #Reshape to the network input shape (3, w, h).\n",
        "        img = np.array([np.transpose(np.float32(original_img), (2, 0, 1))])\n",
        "        \n",
        "        #Get the 512 input weights to the softmax.\n",
        "        class_weights = model.layers[-1].get_weights()[0]\n",
        "        final_conv_layer = get_output_layer(model, \"conv5_3\")\n",
        "        get_output = K.function([model.layers[0].input], \\\n",
        "                    [final_conv_layer.output, \n",
        "        model.layers[-1].output])\n",
        "        [conv_outputs, predictions] = get_output([img])\n",
        "        conv_outputs = conv_outputs[0, :, :, :]\n",
        "\n",
        "        #Create the class activation map.\n",
        "        cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[1:3])\n",
        "        target_class = 1\n",
        "        for i, w in enumerate(class_weights[:, target_class]):\n",
        "                cam += w * conv_outputs[i, :, :]\n",
        "        print \"predictions\", predictions\n",
        "        cam /= np.max(cam)\n",
        "        cam = cv2.resize(cam, (height, width))\n",
        "        heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
        "        heatmap[np.where(cam < 0.2)] = 0\n",
        "        img = heatmap*0.5 + original_img\n",
        "        cv2.imwrite(output_path, img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dHRqp23fGCzq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}